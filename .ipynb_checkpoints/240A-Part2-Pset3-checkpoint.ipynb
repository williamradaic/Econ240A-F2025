{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "807938cd-9f8f-4ada-a9a4-7930d6891e61",
   "metadata": {},
   "source": [
    "# Econ 240A - Part 2 - Problem Set 3\n",
    "## William Radaic\n",
    "## Fall 2025\n",
    "## This version: 25 Nov 2025\n",
    "\n",
    "(Problem 1 based on section code and solutions; Problem 2 solved using ChatGPT to give me hints.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832e54ee-d929-46d7-bdb6-51137f33ea76",
   "metadata": {},
   "source": [
    "# Problem 1 - Bayesian Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fa984c23-06ce-4a90-905f-1baa8c18a053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pubid</th>\n",
       "      <th>avg_earn_2014_to_2018</th>\n",
       "      <th>hgc_ever</th>\n",
       "      <th>asvab</th>\n",
       "      <th>female</th>\n",
       "      <th>black</th>\n",
       "      <th>hispanic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>148921.484038</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.5070</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>138251.525557</td>\n",
       "      <td>14.0</td>\n",
       "      <td>5.8483</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>68974.532647</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.7978</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>47482.812818</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.7012</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>13759.262574</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.2001</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pubid  avg_earn_2014_to_2018  hgc_ever   asvab  female  black  hispanic\n",
       "0      1          148921.484038      16.0  4.5070       1      0         0\n",
       "1      2          138251.525557      14.0  5.8483       0      0         1\n",
       "2      3           68974.532647      16.0  2.7978       1      0         1\n",
       "3      4           47482.812818      13.0  3.7012       1      0         1\n",
       "4      6           13759.262574      14.0  2.2001       1      0         1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## (Based on section code) ##\n",
    "\n",
    "## Setup and loading data\n",
    "\n",
    "\n",
    "# Direct Python to plot all figures inline (i.e., not in a separate window)\n",
    "%matplotlib inline\n",
    "\n",
    "# Load libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Load data\n",
    "nlsy97 = pd.read_csv('data/nlsy97ss.csv')\n",
    "nlsy97['asvab'] = nlsy97['asvab'] / 10; # Normalize AFQT \n",
    "nlsy97.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30d39b7b-b707-4bc0-9435-abd87f195659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_earn_2014_to_2018</th>\n",
       "      <th>LogEarn</th>\n",
       "      <th>hgc_ever</th>\n",
       "      <th>asvab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1606.000000</td>\n",
       "      <td>1606.000000</td>\n",
       "      <td>1606.000000</td>\n",
       "      <td>1606.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>75821.768976</td>\n",
       "      <td>10.929359</td>\n",
       "      <td>14.346824</td>\n",
       "      <td>5.694810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>59827.898795</td>\n",
       "      <td>0.914928</td>\n",
       "      <td>3.011072</td>\n",
       "      <td>2.839744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>58.452994</td>\n",
       "      <td>4.068223</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>38395.238774</td>\n",
       "      <td>10.555689</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>3.377600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>61895.366862</td>\n",
       "      <td>11.033201</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>5.953000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>94180.041510</td>\n",
       "      <td>11.452963</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>8.203425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>383978.885126</td>\n",
       "      <td>12.858343</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       avg_earn_2014_to_2018      LogEarn     hgc_ever        asvab\n",
       "count            1606.000000  1606.000000  1606.000000  1606.000000\n",
       "mean            75821.768976    10.929359    14.346824     5.694810\n",
       "std             59827.898795     0.914928     3.011072     2.839744\n",
       "min                58.452994     4.068223     6.000000     0.000000\n",
       "25%             38395.238774    10.555689    12.000000     3.377600\n",
       "50%             61895.366862    11.033201    14.000000     5.953000\n",
       "75%             94180.041510    11.452963    16.000000     8.203425\n",
       "max            383978.885126    12.858343    20.000000    10.000000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 1.1 Create subsample \n",
    "\n",
    "# Subsample indicator\n",
    "sample = (nlsy97['black']==0) & (nlsy97['hispanic']==0) & (nlsy97['female']==0) & (nlsy97['avg_earn_2014_to_2018']>0)\n",
    "sample.head()\n",
    "\n",
    "# Filtering dataset\n",
    "nlsy97 = nlsy97[sample]\n",
    "\n",
    "# Creating log(earnings)\n",
    "nlsy97['LogEarn'] = np.log(nlsy97['avg_earn_2014_to_2018'])\n",
    "\n",
    "# Creating interaction term (for item 6)\n",
    "nlsy97['hgc_X_asvab'] = nlsy97['hgc_ever']*(nlsy97['asvab'] - 50)\n",
    "\n",
    "# Summary statistics\n",
    "nlsy97[[\"avg_earn_2014_to_2018\",\"LogEarn\",\"hgc_ever\",\"asvab\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "79146209-7944-4990-a4d8-17bf370ef596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>LogEarn</td>     <th>  R-squared:         </th> <td>   0.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.102</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   189.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 26 Nov 2025</td> <th>  Prob (F-statistic):</th> <td>7.92e-41</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:23:17</td>     <th>  Log-Likelihood:    </th> <td> -2048.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1606</td>      <th>  AIC:               </th> <td>   4101.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  1604</td>      <th>  BIC:               </th> <td>   4112.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>         <td>HC0</td>       <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>        <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>    <td>    9.5328</td> <td>    0.107</td> <td>   89.250</td> <td> 0.000</td> <td>    9.323</td> <td>    9.742</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hgc_ever</th> <td>    0.0973</td> <td>    0.007</td> <td>   13.764</td> <td> 0.000</td> <td>    0.083</td> <td>    0.111</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>778.410</td> <th>  Durbin-Watson:     </th> <td>   1.881</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>7568.453</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-2.035</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>12.825</td>  <th>  Cond. No.          </th> <td>    71.7</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors are heteroscedasticity robust (HC0)"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &     LogEarn      & \\textbf{  R-squared:         } &     0.103   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.102   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     189.5   \\\\\n",
       "\\textbf{Date:}             & Wed, 26 Nov 2025 & \\textbf{  Prob (F-statistic):} &  7.92e-41   \\\\\n",
       "\\textbf{Time:}             &     11:23:17     & \\textbf{  Log-Likelihood:    } &   -2048.6   \\\\\n",
       "\\textbf{No. Observations:} &        1606      & \\textbf{  AIC:               } &     4101.   \\\\\n",
       "\\textbf{Df Residuals:}     &        1604      & \\textbf{  BIC:               } &     4112.   \\\\\n",
       "\\textbf{Df Model:}         &           1      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &       HC0        & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                   & \\textbf{coef} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}     &       9.5328  &        0.107     &    89.250  &         0.000        &        9.323    &        9.742     \\\\\n",
       "\\textbf{hgc\\_ever} &       0.0973  &        0.007     &    13.764  &         0.000        &        0.083    &        0.111     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 778.410 & \\textbf{  Durbin-Watson:     } &    1.881  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 7568.453  \\\\\n",
       "\\textbf{Skew:}          &  -2.035 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  12.825 & \\textbf{  Cond. No.          } &     71.7  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors are heteroscedasticity robust (HC0)"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                LogEarn   R-squared:                       0.103\n",
       "Model:                            OLS   Adj. R-squared:                  0.102\n",
       "Method:                 Least Squares   F-statistic:                     189.5\n",
       "Date:                Wed, 26 Nov 2025   Prob (F-statistic):           7.92e-41\n",
       "Time:                        11:23:17   Log-Likelihood:                -2048.6\n",
       "No. Observations:                1606   AIC:                             4101.\n",
       "Df Residuals:                    1604   BIC:                             4112.\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:                  HC0                                         \n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          9.5328      0.107     89.250      0.000       9.323       9.742\n",
       "hgc_ever       0.0973      0.007     13.764      0.000       0.083       0.111\n",
       "==============================================================================\n",
       "Omnibus:                      778.410   Durbin-Watson:                   1.881\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             7568.453\n",
       "Skew:                          -2.035   Prob(JB):                         0.00\n",
       "Kurtosis:                      12.825   Cond. No.                         71.7\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors are heteroscedasticity robust (HC0)\n",
       "\"\"\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 1.2. OLS fit - short regression: LogEarn ~ 1 + hgc_ever\n",
    "\n",
    "short_reg=sm.OLS(nlsy97['LogEarn'],sm.add_constant(nlsy97['hgc_ever'])).fit(cov_type='HC0') # HC0 is heteroskedastic-robust s.e.\n",
    "short_reg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f046ef34-6483-476c-8f60-04109e99721d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>LogEarn</td>     <th>  R-squared:         </th> <td>   0.116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.115</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   107.7</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 26 Nov 2025</td> <th>  Prob (F-statistic):</th> <td>1.33e-44</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:23:17</td>     <th>  Log-Likelihood:    </th> <td> -2036.1</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1606</td>      <th>  AIC:               </th> <td>   4078.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  1603</td>      <th>  BIC:               </th> <td>   4094.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>         <td>HC0</td>       <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>        <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>    <td>    9.6195</td> <td>    0.107</td> <td>   89.771</td> <td> 0.000</td> <td>    9.410</td> <td>    9.830</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hgc_ever</th> <td>    0.0732</td> <td>    0.009</td> <td>    8.010</td> <td> 0.000</td> <td>    0.055</td> <td>    0.091</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>asvab</th>    <td>    0.0457</td> <td>    0.011</td> <td>    4.140</td> <td> 0.000</td> <td>    0.024</td> <td>    0.067</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>780.475</td> <th>  Durbin-Watson:     </th> <td>   1.907</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>7638.672</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-2.040</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>12.875</td>  <th>  Cond. No.          </th> <td>    78.5</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors are heteroscedasticity robust (HC0)"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &     LogEarn      & \\textbf{  R-squared:         } &     0.116   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.115   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     107.7   \\\\\n",
       "\\textbf{Date:}             & Wed, 26 Nov 2025 & \\textbf{  Prob (F-statistic):} &  1.33e-44   \\\\\n",
       "\\textbf{Time:}             &     11:23:17     & \\textbf{  Log-Likelihood:    } &   -2036.1   \\\\\n",
       "\\textbf{No. Observations:} &        1606      & \\textbf{  AIC:               } &     4078.   \\\\\n",
       "\\textbf{Df Residuals:}     &        1603      & \\textbf{  BIC:               } &     4094.   \\\\\n",
       "\\textbf{Df Model:}         &           2      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &       HC0        & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                   & \\textbf{coef} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}     &       9.6195  &        0.107     &    89.771  &         0.000        &        9.410    &        9.830     \\\\\n",
       "\\textbf{hgc\\_ever} &       0.0732  &        0.009     &     8.010  &         0.000        &        0.055    &        0.091     \\\\\n",
       "\\textbf{asvab}     &       0.0457  &        0.011     &     4.140  &         0.000        &        0.024    &        0.067     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 780.475 & \\textbf{  Durbin-Watson:     } &    1.907  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 7638.672  \\\\\n",
       "\\textbf{Skew:}          &  -2.040 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  12.875 & \\textbf{  Cond. No.          } &     78.5  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors are heteroscedasticity robust (HC0)"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                LogEarn   R-squared:                       0.116\n",
       "Model:                            OLS   Adj. R-squared:                  0.115\n",
       "Method:                 Least Squares   F-statistic:                     107.7\n",
       "Date:                Wed, 26 Nov 2025   Prob (F-statistic):           1.33e-44\n",
       "Time:                        11:23:17   Log-Likelihood:                -2036.1\n",
       "No. Observations:                1606   AIC:                             4078.\n",
       "Df Residuals:                    1603   BIC:                             4094.\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:                  HC0                                         \n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          9.6195      0.107     89.771      0.000       9.410       9.830\n",
       "hgc_ever       0.0732      0.009      8.010      0.000       0.055       0.091\n",
       "asvab          0.0457      0.011      4.140      0.000       0.024       0.067\n",
       "==============================================================================\n",
       "Omnibus:                      780.475   Durbin-Watson:                   1.907\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             7638.672\n",
       "Skew:                          -2.040   Prob(JB):                         0.00\n",
       "Kurtosis:                      12.875   Cond. No.                         78.5\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors are heteroscedasticity robust (HC0)\n",
       "\"\"\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 1.3. Long regression: LogEarn ~ 1 + hgc_ever + asvab\n",
    "\n",
    "long_reg=sm.OLS(nlsy97['LogEarn'],sm.add_constant(nlsy97[['hgc_ever','asvab']])).fit(cov_type='HC0')\n",
    "long_reg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26f8db1-211f-4a44-abf0-5484e668b0cf",
   "metadata": {},
   "source": [
    "Yes, the estimate on `hgc_ever` goes down from 0.0973 to 0.0732. This is because schooling and \"ability\" are correlated; in the short regression, the coefficient associated with schooling is picking up some of the effect of ability. This is usually called \"Ommited Variable Bias\" in the literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "88fd7475-9e73-43c9-9c8f-74dfc1d239fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.4. Auxiliary regression: asvab ~ 1 + hgc_ever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9f175ca0-9844-4d90-808a-f63be6afaf91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>asvab</td>      <th>  R-squared:         </th> <td>   0.315</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.315</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   835.3</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 26 Nov 2025</td> <th>  Prob (F-statistic):</th> <td>3.35e-148</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:23:17</td>     <th>  Log-Likelihood:    </th> <td> -3650.7</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1606</td>      <th>  AIC:               </th> <td>   7305.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  1604</td>      <th>  BIC:               </th> <td>   7316.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>         <td>HC0</td>       <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>        <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>    <td>   -1.8995</td> <td>    0.273</td> <td>   -6.962</td> <td> 0.000</td> <td>   -2.434</td> <td>   -1.365</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hgc_ever</th> <td>    0.5293</td> <td>    0.018</td> <td>   28.901</td> <td> 0.000</td> <td>    0.493</td> <td>    0.565</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>42.046</td> <th>  Durbin-Watson:     </th> <td>   1.945</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  33.498</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.272</td> <th>  Prob(JB):          </th> <td>5.32e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.547</td> <th>  Cond. No.          </th> <td>    71.7</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors are heteroscedasticity robust (HC0)"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &      asvab       & \\textbf{  R-squared:         } &     0.315   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.315   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     835.3   \\\\\n",
       "\\textbf{Date:}             & Wed, 26 Nov 2025 & \\textbf{  Prob (F-statistic):} & 3.35e-148   \\\\\n",
       "\\textbf{Time:}             &     11:23:17     & \\textbf{  Log-Likelihood:    } &   -3650.7   \\\\\n",
       "\\textbf{No. Observations:} &        1606      & \\textbf{  AIC:               } &     7305.   \\\\\n",
       "\\textbf{Df Residuals:}     &        1604      & \\textbf{  BIC:               } &     7316.   \\\\\n",
       "\\textbf{Df Model:}         &           1      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &       HC0        & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                   & \\textbf{coef} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}     &      -1.8995  &        0.273     &    -6.962  &         0.000        &       -2.434    &       -1.365     \\\\\n",
       "\\textbf{hgc\\_ever} &       0.5293  &        0.018     &    28.901  &         0.000        &        0.493    &        0.565     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 42.046 & \\textbf{  Durbin-Watson:     } &    1.945  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.000 & \\textbf{  Jarque-Bera (JB):  } &   33.498  \\\\\n",
       "\\textbf{Skew:}          & -0.272 & \\textbf{  Prob(JB):          } & 5.32e-08  \\\\\n",
       "\\textbf{Kurtosis:}      &  2.547 & \\textbf{  Cond. No.          } &     71.7  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors are heteroscedasticity robust (HC0)"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  asvab   R-squared:                       0.315\n",
       "Model:                            OLS   Adj. R-squared:                  0.315\n",
       "Method:                 Least Squares   F-statistic:                     835.3\n",
       "Date:                Wed, 26 Nov 2025   Prob (F-statistic):          3.35e-148\n",
       "Time:                        11:23:17   Log-Likelihood:                -3650.7\n",
       "No. Observations:                1606   AIC:                             7305.\n",
       "Df Residuals:                    1604   BIC:                             7316.\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:                  HC0                                         \n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         -1.8995      0.273     -6.962      0.000      -2.434      -1.365\n",
       "hgc_ever       0.5293      0.018     28.901      0.000       0.493       0.565\n",
       "==============================================================================\n",
       "Omnibus:                       42.046   Durbin-Watson:                   1.945\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               33.498\n",
       "Skew:                          -0.272   Prob(JB):                     5.32e-08\n",
       "Kurtosis:                       2.547   Cond. No.                         71.7\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors are heteroscedasticity robust (HC0)\n",
       "\"\"\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auxi_reg=sm.OLS(nlsy97['asvab'],sm.add_constant(nlsy97['hgc_ever'])).fit(cov_type='HC0')\n",
    "auxi_reg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7691e1-fe74-4b2b-92c6-0761881b0c0f",
   "metadata": {},
   "source": [
    "**1.5.** We use the FWL theorem to reconstruct the long-regression coefficient:\n",
    "$$ \\delta = \\beta + \\Pi \\gamma $$\n",
    "where $\\delta$ is the short-regression coefficient for `hgc_ever`; $\\beta$ is the long-regression coefficient for `hgc_ever`; $\\gamma$ is the long-regression coefficient on `asvab`; and $\\Pi$ is the auxiliary-regression coefficient for `hgc_ever`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ddc781b8-7c2b-4a99-9816-c2a05e0e3c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta = 0.09734526191764158\n"
     ]
    }
   ],
   "source": [
    "long_reg_FWL = long_reg.params['hgc_ever']+auxi_reg.params['hgc_ever']*long_reg.params['asvab']\n",
    "print(\"delta =\", long_reg_FWL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf20aaa-3a07-4e9c-b98b-3250336cb447",
   "metadata": {},
   "source": [
    "By the FWL theorem, the coefficient should be equal (up to rounding/floating point errors). Checking this below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "774be12f-654c-4a68-b1ca-9f4070060d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isclose(long_reg.params['hgc_ever']+auxi_reg.params['hgc_ever']*long_reg.params['asvab'],short_reg.params['hgc_ever'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "edf834d9-9c6b-42cf-857a-fb77fd85d990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>LogEarn</td>     <th>  R-squared:         </th> <td>   0.117</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.115</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   76.32</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 26 Nov 2025</td> <th>  Prob (F-statistic):</th> <td>3.86e-46</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:24:40</td>     <th>  Log-Likelihood:    </th> <td> -2035.7</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1606</td>      <th>  AIC:               </th> <td>   4079.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  1602</td>      <th>  BIC:               </th> <td>   4101.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>         <td>HC0</td>       <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>          <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>       <td>    9.8137</td> <td>    0.272</td> <td>   36.122</td> <td> 0.000</td> <td>    9.281</td> <td>   10.346</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hgc_ever</th>    <td>    0.1866</td> <td>    0.132</td> <td>    1.416</td> <td> 0.157</td> <td>   -0.072</td> <td>    0.445</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hgc_X_asvab</th> <td>    0.0026</td> <td>    0.003</td> <td>    0.850</td> <td> 0.395</td> <td>   -0.003</td> <td>    0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>asvab</th>       <td>    0.0096</td> <td>    0.043</td> <td>    0.224</td> <td> 0.823</td> <td>   -0.075</td> <td>    0.094</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>779.561</td> <th>  Durbin-Watson:     </th> <td>   1.908</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>7612.557</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-2.037</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>12.857</td>  <th>  Cond. No.          </th> <td>7.51e+03</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors are heteroscedasticity robust (HC0)<br/>[2] The condition number is large, 7.51e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &     LogEarn      & \\textbf{  R-squared:         } &     0.117   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.115   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     76.32   \\\\\n",
       "\\textbf{Date:}             & Wed, 26 Nov 2025 & \\textbf{  Prob (F-statistic):} &  3.86e-46   \\\\\n",
       "\\textbf{Time:}             &     11:24:40     & \\textbf{  Log-Likelihood:    } &   -2035.7   \\\\\n",
       "\\textbf{No. Observations:} &        1606      & \\textbf{  AIC:               } &     4079.   \\\\\n",
       "\\textbf{Df Residuals:}     &        1602      & \\textbf{  BIC:               } &     4101.   \\\\\n",
       "\\textbf{Df Model:}         &           3      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &       HC0        & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                       & \\textbf{coef} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}         &       9.8137  &        0.272     &    36.122  &         0.000        &        9.281    &       10.346     \\\\\n",
       "\\textbf{hgc\\_ever}     &       0.1866  &        0.132     &     1.416  &         0.157        &       -0.072    &        0.445     \\\\\n",
       "\\textbf{hgc\\_X\\_asvab} &       0.0026  &        0.003     &     0.850  &         0.395        &       -0.003    &        0.008     \\\\\n",
       "\\textbf{asvab}         &       0.0096  &        0.043     &     0.224  &         0.823        &       -0.075    &        0.094     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 779.561 & \\textbf{  Durbin-Watson:     } &    1.908  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 7612.557  \\\\\n",
       "\\textbf{Skew:}          &  -2.037 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  12.857 & \\textbf{  Cond. No.          } & 7.51e+03  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors are heteroscedasticity robust (HC0) \\newline\n",
       " [2] The condition number is large, 7.51e+03. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                LogEarn   R-squared:                       0.117\n",
       "Model:                            OLS   Adj. R-squared:                  0.115\n",
       "Method:                 Least Squares   F-statistic:                     76.32\n",
       "Date:                Wed, 26 Nov 2025   Prob (F-statistic):           3.86e-46\n",
       "Time:                        11:24:40   Log-Likelihood:                -2035.7\n",
       "No. Observations:                1606   AIC:                             4079.\n",
       "Df Residuals:                    1602   BIC:                             4101.\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:                  HC0                                         \n",
       "===============================================================================\n",
       "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------\n",
       "const           9.8137      0.272     36.122      0.000       9.281      10.346\n",
       "hgc_ever        0.1866      0.132      1.416      0.157      -0.072       0.445\n",
       "hgc_X_asvab     0.0026      0.003      0.850      0.395      -0.003       0.008\n",
       "asvab           0.0096      0.043      0.224      0.823      -0.075       0.094\n",
       "==============================================================================\n",
       "Omnibus:                      779.561   Durbin-Watson:                   1.908\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             7612.557\n",
       "Skew:                          -2.037   Prob(JB):                         0.00\n",
       "Kurtosis:                      12.857   Cond. No.                     7.51e+03\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors are heteroscedasticity robust (HC0)\n",
       "[2] The condition number is large, 7.51e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 1.6. Including interaction term in the long regression\n",
    "\n",
    "inter_reg=sm.OLS(nlsy97['LogEarn'],sm.add_constant(nlsy97[['hgc_ever','hgc_X_asvab','asvab']])).fit(cov_type='HC0')\n",
    "inter_reg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b914a0-6727-4d2b-97c0-e7777337f74b",
   "metadata": {},
   "source": [
    "**(a)** The interpretation of $\\beta_0$ is: fixing a level of ability (AFQT) and ignoring the interaction effect between higher AFQT and higher education, an increase of 1 year of schooling increases earnings by 18%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1938cd-9731-4eb6-9836-081f12557648",
   "metadata": {},
   "source": [
    "# Problem 2 - Metropolis-Hastings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ca78d0-2667-41ea-9b2d-8895f757c12b",
   "metadata": {},
   "source": [
    "**2.1.** The Metropolis-Hastings (MH) algorithm is a computationally efficient procedure to approximate posterior distributions when exact computations are intractable. \n",
    "In the problem at hand, we have specified prior subjective uncertainty over the parameters $\\phi$ and $\\sigma^2$, and want to use the data to compute the posterior distribution of these parameters.\n",
    "In order to construct this posterior distribution, we need to approximate more draws from this posterior, since the observed data corresponds to just a single draw.\n",
    "The key reason why the procedure works is that the Markov Chain induced by the algorithm converges to a stationary distribution as $s$ grows---i.e., converges to the posterior distribution. Thus, we can approximate draws from the posterior and construct its distribution.\n",
    "$\\theta'$ is always accepted whenever $\\alpha > 1$---i.e., whenever the likelihood ratio for the new parameter is larger than 1. This means that the new parameter $\\theta'$ is an improvement (in terms of likelihood) relative to $\\theta$. If $\\alpha < 1$, the new parameter will be adopted with positive probability given by the CDF of a Uniform(0,1) evaluated at $\\alpha$; that is, whenever $\\theta'$ leads to a reduction in likelihood, it can still be adopted with some positive probability. The reasoning behind this decision rule is that it allows for the algorithm to visit all the parameter space and not get stuck in local minima."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c6a5f83b-f0d4-4af1-8289-4c4364041385",
   "metadata": {},
   "source": [
    "**2.2.**\n",
    "\n",
    "**(a)** \n",
    "$\\kappa_\\phi$ scales the variance of the distribution $\\phi'\\mid \\theta$, from which new values of $\\phi$ will be drawn. A higher value of $\\kappa_\\phi$ corresponds to a more dispersed distribution, which means draws $\\phi'$ are less likely to be concentrated near the old parameter $\\phi$.\n",
    "$\\kappa_\\sigma$ scales the shape and the scale of the distribution of new values of $\\sigma^{2\\prime} \\mid \\theta$. Again, a higher value of $\\kappa_\\phi$ corresponds to a more dispersed distribution, which means draws $\\sigma^{2\\prime}$ are less likely to be concentrated near the old parameter $\\sigma^2$. In other words, increasing $\\kappa$ in the implementation of the algorithm leads to a more dispersed search process, visiting more regions of the parameter space with higher frequency. In terms of the Markov Chain, it means the transition matrix will be less sparse, in the sense of having more equally distributed values across the entire matrix. **[[[???]]]**\n",
    "\n",
    "**(b)** \n",
    "Since $\\phi'$ and $\\sigma^{2\\prime}$ are drawn indepdendently, the density $g(\\theta' \\mid \\theta)$ is equal to \n",
    "$$ g(\\theta' \\mid \\theta) = g(\\phi' \\mid \\theta) g(\\sigma^{2\\prime} \\mid \\theta) $$\n",
    "where $g(\\phi' \\mid \\theta)$ and $g(\\sigma^{2\\prime} \\mid \\theta)$ are the pdfs associated with $\\phi' \\mid \\theta$ and $\\sigma^{2\\prime} \\mid \\theta$ respectively.\n",
    "\n",
    "Since $\\phi'\\mid \\theta \\sim \\mathcal{N}(\\phi, \\kappa_\\phi I_k)$, we can write for the case of the 4th and 5th draws:\n",
    "$$g(\\phi^{(5)} \\mid \\theta^{(4)}) = (2 \\pi)^{-k / 2} \\kappa_\\phi^{-k / 2} \\exp \\left(-\\frac{1}{2}(\\phi^{(5)}-\\phi^{(4)})^{\\mathrm{T}} \\kappa_\\phi^{-1} (\\phi^{(5)}-\\phi^{(4)})\\right).$$\n",
    "\n",
    "Similarly, for $\\sigma^{2\\prime} \\mid \\theta$ in the case at hand we have:\n",
    "$$\n",
    "g(\\sigma^{2(5)} \\mid \\theta^{(4)}) \n",
    "= \\frac{\n",
    "\\kappa_\\sigma^{\\,\\kappa_\\sigma \\sigma^{2(4)}}\n",
    "}{\n",
    "\\Gamma(\\kappa_\\sigma \\sigma^{2(4)})\n",
    "}\n",
    "\\,\n",
    "(\\sigma^{2(5)})^{\\kappa_\\sigma \\sigma^{2(4)} - 1}\n",
    "\\exp\\!\\left( -\\kappa_\\sigma \\sigma^{2(5)} \\right).$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Multiplying both expressions above gives us the final expression for $g(\\theta' \\mid \\theta)$:\n",
    "$$g(\\theta^{(5)} \\mid \\theta^{(4)}) = (2 \\pi)^{-k / 2} \\kappa_\\phi^{-k / 2} \\exp \\left(-\\frac{1}{2}(\\phi^{(5)}-\\phi^{(4)})^{\\mathrm{T}} \\kappa_\\phi^{-1} (\\phi^{(5)}-\\phi^{(4)})\\right) \\frac{\n",
    "\\kappa_\\sigma^{\\,\\kappa_\\sigma \\sigma^{2(4)}}\n",
    "}{\n",
    "\\Gamma(\\kappa_\\sigma \\sigma^{2(4)})\n",
    "}\n",
    "\\,\n",
    "(\\sigma^{2(5)})^{\\kappa_\\sigma \\sigma^{2(4)} - 1}\n",
    "\\exp\\!\\left( -\\kappa_\\sigma \\sigma^{2(5)} \\right)$$\n",
    "\n",
    "\n",
    "**(c)** \n",
    "Since the normal pdf is symmetric, the terms related to $\\phi \\mid \\theta$ cancel out. Thus, this correction term is just a ratio of the Gamma distributions. After algebra, we get:\n",
    "\n",
    "$$\\frac{g(\\theta \\mid \\theta')}{g(\\theta' \\mid \\theta)}\n",
    "=\n",
    "\\frac{\\Gamma\\!\\big(\\kappa_\\sigma \\sigma^{2(4)}\\big)}\n",
    "     {\\Gamma\\!\\big(\\kappa_\\sigma \\sigma^{2(5)}\\big)}\n",
    "\\,\n",
    "\\kappa_\\sigma^{\\,\\kappa_\\sigma \\big(\\sigma^{2(5)} - \\sigma^{2(4)}\\big)}\n",
    "\\,\n",
    "\\frac{\n",
    "\\big(\\sigma^{2(4)}\\big)^{\\kappa_\\sigma \\sigma^{2(5)} - 1}\n",
    "}{\n",
    "\\big(\\sigma^{2(5)}\\big)^{\\kappa_\\sigma \\sigma^{2(4)} - 1}\n",
    "}\n",
    "\\,\n",
    "\\exp\\!\\left[-\\kappa_\\sigma\\big(\\sigma^{2(4)} - \\sigma^{2(5)}\\big)\\right].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb6e3eb-5916-442b-903f-b8b3328cd08a",
   "metadata": {},
   "source": [
    "**2.3.**\n",
    "We start by factoring the posterior ratio as \n",
    "$$\n",
    "\\frac{\\pi(\\theta' \\mid Y)}{\\pi(\\theta \\mid Y)}\n",
    "=\n",
    "\\frac{\n",
    "p(Y \\mid \\phi',\\sigma'^2)\\,p(\\phi' \\mid \\sigma'^2)\\,p(\\sigma'^2)\n",
    "}{\n",
    "p(Y \\mid \\phi,\\sigma^2)\\,p(\\phi \\mid \\sigma^2)\\,p(\\sigma^2)\n",
    "},$$\n",
    "where the first term is the likelihood ratio; the second term is the (Normal) prior on $\\phi$, and the third term is the Inverse-Gamma prior on $\\sigma^2$.\n",
    "\n",
    "Given that these are ratios, the constant terms in each of the pdfs cancel out. Thus, after algebra, we have:\n",
    "$$\n",
    "\\frac{\\pi(\\theta' \\mid Y)}{\\pi(\\theta \\mid Y)}=\n",
    "\\left(\\frac{\\sigma'^2}{\\sigma^2}\\right)^{-\\frac{n+K}{2}}\n",
    "\\left(\\frac{\\sigma^2}{\\sigma'^2}\\right)^{\\alpha+1}\n",
    "\\exp\\!\\left[\n",
    "  -\\frac{1}{2\\sigma'^2}\\big(\\|Y - X\\phi'\\|^2 + (\\phi' - \\bar\\phi)'\\Lambda(\\phi' - \\bar\\phi)\\big)\n",
    "  +\\frac{1}{2\\sigma^2}\\big(\\|Y - X\\phi\\|^2 + (\\phi - \\bar\\phi)'\\Lambda(\\phi - \\bar\\phi)\\big)\n",
    "  -\\frac{\\beta}{\\sigma'^2} + \\frac{\\beta}{\\sigma^2}\n",
    "\\right]\n",
    "$$\n",
    "where $(\\alpha = \\nu_0/2,\\ \\beta = \\nu_0 s_0^2/2)$.\n",
    "Interpreting the expression above: $\\|Y - X\\phi'\\|^2$ and $\\|Y - X\\phi\\|^2$ come from the LR (with no Var-Cov matrix since we assume homoskedasticity); $(\\phi' - \\bar\\phi)'\\Lambda(\\phi' - \\bar\\phi)\\big)$ come from the prior on $\\phi$, with associated Var-Cov matrix $\\Lambda$; and the $\\beta$ fractions come from the Inverse-Gamma prior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de83a83d-7a00-48ba-8f0c-5c4de533b447",
   "metadata": {},
   "source": [
    "**2.4.**\n",
    "**(a)** The hyperparameters determine our prior subjective uncertainty regarding the parameters of interest $\\phi$. $\\bar{\\phi}$ is the mean of the prior (Normal) distribution: it represents the average value of $\\phi$ we have in our heads before conducting the analysis. $\\Lambda^{-1}$ represents the covariances between different dimensions of the parameter $\\phi$. For example, we may expect the effects of schooling and AFQT on earnings to be correlated, such that the prior uncertainty for $\\phi$ carries this correlation structure. $v_0$ and $s_0^2$ determine the shape and scale of the Inverse-Gamma distribution. Larger values of $s_0$ correspond to a more concentrated prior on variance $\\sigma^2$; larger values of $v_0$ (focusing on its effect on shape) shift more density to lower values of $\\sigma^2$. Thus: large $v_0$ means that our prior is that $U$ has low variance; high $s_0$ means our prior itself is more concentrated. **[[[???]]]**\n",
    "\n",
    "**(b)** When $S$ is large, we approach the stationary distribution of the Markov Chain --- which is unique due to ergodicity. Thus for large $S$, as $s$ increases, the prior will play a negligible role in the evolution of the Markov Chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6ec691-a35f-43c3-a8de-5cc7cd50ad53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9dbee6-e933-4b47-92b0-c48d20ae28ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f040637-a607-4a14-b028-9a9723aede97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca77dca-e818-4bfc-af43-cf3dfe6c7d75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
